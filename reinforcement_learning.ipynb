{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intro to Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this assignment you will learn the basics of reinforcement learning (RL), as well how to train a DQN RL model.\n",
        "\n",
        "A lot of this information, text, and code is directly pooled from the following sources\n",
        "\n",
        "Credits:\n",
        "* Hugging Face Deep RL course [source](https://huggingface.co/learn/deep-rl-course/unit0/introduction)\n",
        "* Gymnasium-Colaboratory-Starter [source](https://github.com/hom-bahrani/Gymnasium-Colaboratory-Starter)\n",
        "* PyTorch RL Tutorials [source](https://pytorch.org/tutorials/)\n",
        "* Wikipedia RL Page [source](https://en.wikipedia.org/wiki/Reinforcement_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, install and import the packages and libraries you will need throughout this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MGHP8DHMBKp",
        "outputId": "b8a735b9-32d3-416e-e7bf-364e88b14fca"
      },
      "outputs": [],
      "source": [
        "# install gymnasium packages\n",
        "%pip install gymnasium\n",
        "%pip install gymnasium[classic-control]\n",
        "%pip install swig\n",
        "\n",
        "# install virtual display packages\n",
        "%pip install pyvirtualdisplay\n",
        "%pip install moviepy\n",
        "%pip install --upgrade moviepy\n",
        "\n",
        "# install stable baselines3\n",
        "%pip install stable-baselines3\n",
        "\n",
        "# install snntorch\n",
        "%pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the Package Index\n",
        "!sudo apt update --quiet\n",
        "\n",
        "# Install xvfb\n",
        "!sudo apt-get install -y xvfb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzeSyt4-OPR_"
      },
      "outputs": [],
      "source": [
        "# gymnasium imports\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers.record_video import RecordVideo\n",
        "\n",
        "# video imports\n",
        "import glob\n",
        "import uuid\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "# from IPython import display as ipythondisplay\n",
        "\n",
        "# ml imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# plot imports\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import snntorch.spikeplot as splt\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "# utility imports\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import shutil\n",
        "import random\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is Reinforcement Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"images/RL_process.jpg\" width=\"800\" height=\"500\">\n",
        "\n",
        "Along with Supervised Learning and Unsupervised Learning, Reinforcement Learning is one of the 3 basic machine learning paradigms. The idea behind Reinforcement Learning is that an agent (an AI) will learn from the environment by interacting with it (through trial and error) and receiving rewards (negative or positive) as feedback for performing actions.\n",
        "\n",
        "You can think of it like playing a video game where you get points for collecting coins and you lose points for getting hit by enemies. This \"trains\" you to collect coins, while avoiding enemies. Of course, games get a lot more complicated than that which is why one of the main challenges of RL is figuring out how to reward or punish the agent. That is to say, we use the [carrot and the stick](https://en.wikipedia.org/wiki/Carrot_and_stick).\n",
        "\n",
        "The diagram describing how a RL agent interacts with a environment can be interpretated as such:\n",
        "\n",
        "1. The Agent/Model/AI receives a **state $S_t$** (the info the agent receieves from the environment at time t)\n",
        "2. Based on that **state** $S_t$ the agent takes an *action* $A_t$\n",
        "3. The environment goes to a new **state** $S_{t+1}$\n",
        "4. The environment gives some **reward** $R_{t+1}$ to the agent\n",
        "5. repeat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Defining Cumulative Reward\n",
        "\n",
        "The reward is fundamental in RL because it’s the only feedback for the agent. Thanks to it, our agent knows if the action taken was good or not.\n",
        "\n",
        "Where\n",
        "* $R()$: `Return` is the cumulative reward function\n",
        "* $\\tau$: `Trajectory` represented by *tau* is a sequence of `states` and `actions`\n",
        "* $\\gamma$: `Discount Rate` represented by *gamma* between 0-1\n",
        "* $r_t$: `reward` at time step t\n",
        "\n",
        "The cumulative reward is represented by the function\n",
        "\n",
        "$R(\\tau) = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 r_{t+4} + ...$\n",
        "\n",
        "or \n",
        "\n",
        "$R(\\tau) = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$\n",
        "\n",
        "An important insight here is that $\\gamma$ is exponentiated, this means a smaller $\\gamma$ causes a bigger `discount` and rewards at farther timesteps are `discounted` more. So, the smaller the $\\gamma$ the less the agent cares more about closer/short-term rewards, and the bigger $\\gamma$ the more the agent cares about farther/long-term rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Defining Policy\n",
        "\n",
        "You can think of `policy`, $\\pi$, as the mind of the agent\n",
        "\n",
        "This `policy` is the function we want to learn, our goal is to find the `optimal policy` $\\pi^*$, the policy that maximizes expected return when the agent acts according to it. We find this $\\pi^*$ through training. \n",
        "\n",
        "Note: $\\pi^*$ is pronounced \"pie-star\" and in algorithms \"*\" (star) often denotes optimality\n",
        "\n",
        "There are two types of policies:\n",
        "\n",
        "1. Deterministic: a policy at a given state will always return the same action.\n",
        "    * $\\pi(s) = a$\n",
        "2. Stochastic: outputs a probability distribution over actions.\n",
        "    * $\\pi(s|a) = P[A|s]$\n",
        "        * $|$ means given\n",
        "        * $P[]$ means probability distribution\n",
        "        * $P[A|s]$ means a probability distribution over a set of actions given a state\n",
        "\n",
        "There are two approaches to **train** our agent to find this optimal policy π*:\n",
        "\n",
        "1. Directly, by teaching the agent to learn which action to take, given the current state: **Policy-Based Methods**.\n",
        "2. Indirectly, teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states: **Value-Based Methods**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Creating an Environment\n",
        "\n",
        "Now that you understand what a Reinforcement Learning agent is and does. Lets create an environment for our prospective RL agent to interact with.\n",
        "\n",
        "To do that we will be using the environment library [Gymnasium](https://gymnasium.farama.org/) which was developed by OpenAI and handed off to the Farama Foundation to maintain and update."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*GitHub codespaces doesn't have an inbuilt display, create a virtual display instead.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruR-EszrTfDX",
        "outputId": "793fb07e-1331-419d-ed03-6cf07f2254c0"
      },
      "outputs": [],
      "source": [
        "# install a virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first virtual environment you will be using is the `'CartPole-v1'` environment. This is a virtual version of the real life Cart Pole/Inverted Pendulum task, here is a short fun [video](https://youtu.be/nOSTzpA0nGk?si=mFYYwhLhTb5hcrpB) for you to see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Understanding Gymnasium\n",
        "Remember that at each step:\n",
        "- Our Agent receives a **state ($S_0$)** from the **Environment** — i.e. we receive the first frame of our game (Environment).\n",
        "- Based on that **state ($S_0$),** the Agent takes an **action ($A_0$)** — i.e. our Agent will move to the right.\n",
        "- The environment transitions to a **new** **state ($S_1$)** — i.e. new frame.\n",
        "- The environment gives some **reward ($R_1$)** to the Agent — i.e. we’re not dead *(Positive Reward +1)*.\n",
        "\n",
        "\n",
        "With Gymnasium:\n",
        "\n",
        "1. We create our environment using `gymnasium.make()`\n",
        "2. We reset the environment to its initial state with `observation = env.reset()`\n",
        "\n",
        "At each step:\n",
        "\n",
        "3. Get an action using our model (in our example we take a random action)\n",
        "4. Using `env.step(action)`, we perform this action in the environment and get\n",
        "    - `observation`: The new state ($s_{t+1}$)\n",
        "    - `reward`: The reward we get after executing the action\n",
        "    - `terminated`: Indicates if the episode terminated (agent reach the terminal state)\n",
        "    - `truncated`: Introduced with this new version, it indicates a timelimit or if an agent go out of bounds of the environment for instance.\n",
        "    - `info`: A dictionary that provides additional information (depends on the environment).\n",
        "\n",
        "For more explanations check this 👉 https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
        "\n",
        "If the episode is terminated:\n",
        "- We reset the environment to its initial state with `observation = env.reset()`\n",
        "\n",
        "#### 2.2 Task Types\n",
        "There are two types of task: `Episodic Tasks` and `Continuous Tasks`\n",
        "\n",
        "1. Episodic Task\n",
        "    - In this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States.\n",
        "2. Continuous task\n",
        "    - These are tasks that continue forever (no terminal state). In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment.\n",
        "    - For instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1  &emsp; <span style=\"color:green\">(1 Point)</span>\n",
        "\n",
        "In gymnasium the environments are set up as episodic tasks. A good practice is to look at documentation before using an environment to know its properties. If `'CartPole-v1'` is episodic, that means it must have termination conditions. List `'CartPole-v1'`'s termination conditions below.\n",
        "\n",
        "Link to documentation -> https://gymnasium.farama.org/environments/classic_control/cart_pole/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>\n",
        "\n",
        "1.\n",
        "2.\n",
        "3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3 Default Properties of Gymnasium's Cart Pole environment \n",
        "\n",
        "##### **Action Space (The set of possible actions of the model)**\n",
        "\n",
        "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
        "\n",
        "0: Push cart to the left\n",
        "\n",
        "1: Push cart to the right\n",
        "\n",
        "Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
        "\n",
        "##### **Observation Space (The subset of the state the actor observes)**\n",
        "\n",
        "The observation is a ndarray with shape (4,) with the values corresponding to the following positions and velocities:\n",
        "\n",
        "<img src=\"images/observation_space.png\" width=\"700\" height=\"200\">\n",
        "\n",
        "##### **Rewards**\n",
        "\n",
        "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 500 for v1 and 200 for v0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How to access the above values\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n",
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.4 Gymnasium Interactive Loop\n",
        "\n",
        "Below is the code for a simple interactive loop where instead of a agent predicting a action we simply sample a random action. Look at what happens to the cartpole when random actions are chosen, and specifically note how long the episode lasts (duration) which the model will be learning to improve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WqjS0z9Nvbm",
        "outputId": "c7c05a60-fe49-4157-fec5-7c8192196dd4"
      },
      "outputs": [],
      "source": [
        "init_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "folder_name = os.path.join('video', 'CartPole-v1', f'{uuid.uuid4()}')\n",
        "\n",
        "# Use the wrapper environment RecordVideo() to record video\n",
        "env = RecordVideo(init_env, folder_name, disable_logger=True)\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1iWqf2NO4g5"
      },
      "outputs": [],
      "source": [
        "def show_video(folder_name, episode='last'):\n",
        "    mp4list = glob.glob(f'{folder_name}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        sorted_mp4list = sorted(mp4list, key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
        "        if episode == 'last':\n",
        "            mp4 = sorted_mp4list[-1]\n",
        "        elif episode == 'first':\n",
        "            mp4 = sorted_mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                    </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "CD7we0ysR4DN",
        "outputId": "65cc1009-7c2b-4e38-989b-b0bcd0580ef6"
      },
      "outputs": [],
      "source": [
        "show_video(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the videos\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(folder_name):\n",
        "    # Remove the directory\n",
        "    shutil.rmtree(folder_name)\n",
        "    print(\"Directory deleted successfully.\")\n",
        "else:\n",
        "    print(\"Directory does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZwPeGNW9HI"
      },
      "source": [
        "## 3. Deep Q Networks\n",
        "\n",
        "There are many different types of algorithms/models for RL, which you can do research on in your free time, but today we will be introducing you to Deep Q Networks as we think it strikes a nice balance of being easy to explain, but complicated enough to be interesting.\n",
        "\n",
        "<img src=\"images/deep.jpg\" width=\"700\" height=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Approximating $\\pi^*$ with a Q-function\n",
        "\n",
        "Q-learning in general are a set of **value based methods** (which we mentioned earlier), meaning instead of training on the policy directly they train to find an optimal value function which leads to having an optimal policy.\n",
        "\n",
        "In this case the value function the model trains on is called the Q-function, an **action-value function** that determines the value of being at a particular state and taking a specific action at that state. The *link between value and policy* is represented in the equation here:\n",
        "\n",
        "$$\\pi^* = \\underset{a}{\\text{argmax}}\\ Q^*(s,a)$$\n",
        "\n",
        "### 3.2 Maths\n",
        "\n",
        "### 3.2.1 Bellman Equation\n",
        "\n",
        "Directly computing the cumulative reward for all future timesteps for a given state can be a computationally competitive task if you are doing it at each action. Which is why instead we use the **Bellman Equation** which is a recursive equation that considers the value of a state as the immediate reward and the discounted value of the state that follows.\n",
        "\n",
        "<img src=\"images/bellman4.jpg\" width=\"700\" height=\"380\">\n",
        "\n",
        "Remember:\n",
        "* $\\gamma$: `Discount Rate` represented by *gamma* between 0-1\n",
        "\n",
        "\n",
        "### 3.2.2 Temporal Difference Learning Approach\n",
        "\n",
        "There are two main types of strategies:\n",
        "* Monte Carlo Approach - Learning after each episode\n",
        "* Temporal Difference Learning Approach - Learning after each step\n",
        "\n",
        "Q-Learning uses a TD approach which includes the Bellman Equation\n",
        "\n",
        "<img src=\"images/TD-1.jpg\" width=\"700\" height=\"400\">\n",
        "\n",
        "### 3.2.3 Epsilon Greedy Algorithm\n",
        "\n",
        "In RL it is importat to have a policy that governs the **exploitatio-exploration tradeoff**. That is do we exploit our prior knowledge gained during learning and move according to what we currently have calculated as the best action for a given state or do we explore and select a different action in the hopes of finding a better route/set of actions?\n",
        "\n",
        "We use an **epsilon greedy policy** where $\\epsilon$ represents the probability of exploration (selecting a random action) and $1-\\epsilon$ represents the probability of exploitation. $\\epsilon$ is a number from 1-0.\n",
        "\n",
        "At the beginning As Q gets better and better with progressive training we slowly decrease $\\epsilon$ more and more according to a decay rate. This changes the policy to go from a higher ratio of exploration, to a higher ratio of exploitation.\n",
        "\n",
        "### 3.2.4 Q-Learning\n",
        "\n",
        "Bringing that all together we get the Q-Learning algorithm\n",
        "\n",
        "<img src=\"images/Q-learning-2.jpg\" width=\"700\" height=\"400\">\n",
        "\n",
        "### 3.2.5 Deep Q-Learning (DQN)\n",
        "\n",
        "Deep Q-Learning uses a deep neural network to approximate the different Q-values for each possible action at a state (value-function estimation).\n",
        "\n",
        "The difference is that, during the training phase, instead of updating the Q-value of a state-action pair directly as we have done with Q-Learning, in Deep Q-Learning, we create a loss function that compares our Q-value prediction and the Q-target and uses gradient descent to update the weights of our Deep Q-Network to approximate our Q-values better.\n",
        "\n",
        "<img src=\"images/q-ex-5.jpg\" width=\"700\" height=\"400\">\n",
        "\n",
        "<img src=\"images/Q-target.jpg\" width=\"700\" height=\"400\">\n",
        "\n",
        "* Where $\\delta$ represents temporal difference error, the equation looks like this:\n",
        "\n",
        "$$\\delta = Q(s, a) - (r + \\gamma \\max_{a}' Q(s', a'))$$\n",
        "\n",
        "* To minimize this error we use the [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss). The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large.\n",
        "\n",
        "* There is also an additional feature of an **experience replay** which has two functions:\n",
        "    1. Make **more efficient use of the experiences** during the training. Usually, in online reinforcement learning, the agent interacts with the environment, gets experiences (state, action, reward, and next state), learns from them (updates the neural network), and discards them. This is not efficient.\n",
        "\n",
        "    2. Avoid forgetting previous experiences (aka catastrophic interference, or catastrophic forgetting) and reduce the correlation between experiences.\n",
        "        * **catastrophic forgetting**: The problem we get if we give sequential samples of experiences to our neural network is that it tends to forget the previous experiences as it gets new experiences. For instance, if the agent is in the first level and then in the second, which is different, it can forget how to behave and play in the first level.\n",
        "\n",
        "Here is the pseudocode for DQN:\n",
        "\n",
        "<img src=\"images/sampling-training.jpg\" width=\"700\" height=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Train a DQN\n",
        "\n",
        "The following is the code from here: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        " \n",
        "If you want more detail on the specifics of the code, we encouorage you to read through it.\n",
        "\n",
        "Now that you understand the basic overview of a DQN its time to actually train one on the `'CartPole-v1'` Gymnasium environment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiaXSBipW8lw"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6pbi-BwToeQ"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTX4-jUBWSJT"
      },
      "outputs": [],
      "source": [
        "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
        "# GAMMA is the discount factor as mentioned in the previous section\n",
        "# EPS_START is the starting value of epsilon\n",
        "# EPS_END is the final value of epsilon\n",
        "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "# TAU is the update rate of the target network\n",
        "# LR is the learning rate of the ``AdamW`` optimizer\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "# Get the number of state observations\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "        else:\n",
        "            ipythondisplay.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWXFcjLsWhNE"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "0sVkpLBjXZZz",
        "outputId": "1ed8f913-7652-4b9d-a37b-a0a40afd90d1"
      },
      "outputs": [],
      "source": [
        "def train_loop(environment, episodes):\n",
        "\n",
        "    num_episodes = episodes\n",
        "    env = environment\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        # Initialize the environment and get its state\n",
        "        state, info = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        for t in count():\n",
        "            action = select_action(state)\n",
        "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if terminated:\n",
        "                next_state = None\n",
        "            else:\n",
        "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            # Store the transition in memory\n",
        "            memory.push(state, action, next_state, reward)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "            # Perform one step of the optimization (on the policy network)\n",
        "            optimize_model()\n",
        "\n",
        "            # Soft update of the target network's weights\n",
        "            # θ′ ← τ θ + (1 −τ )θ′\n",
        "            target_net_state_dict = target_net.state_dict()\n",
        "            policy_net_state_dict = policy_net.state_dict()\n",
        "            for key in policy_net_state_dict:\n",
        "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "            target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "            if done:\n",
        "                episode_durations.append(t + 1)\n",
        "                plot_durations()\n",
        "                break\n",
        "\n",
        "    print('Complete')\n",
        "    plot_durations(show_result=True)\n",
        "    plt.ioff()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make a new environment\n",
        "init_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "folder_name = os.path.join('video', 'CartPole-v1', f'{uuid.uuid4()}')\n",
        "env = RecordVideo(init_env, folder_name, disable_logger=True)\n",
        "\n",
        "# call the training loop\n",
        "train_loop(env, 500)\n",
        "\n",
        "# close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_video(folder_name, episode = 'first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "gTTuNd_nX3dh",
        "outputId": "cd255b39-551c-4917-fbc2-daf5eba3e6b7"
      },
      "outputs": [],
      "source": [
        "show_video(folder_name, episode = 'last')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2 &emsp; <span style=\"color:green\">(1 Point)</span>\n",
        "Has the model learned? What metrics did you use to determine that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3 (Understanding the code)&emsp; <span style=\"color:green\">(6 Points Total)</span>\n",
        "Hint: Use this as reference -> https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "3A. Copy and paste the section of the code where the epsilon greedy algorithm is implemented. Use 3 backticks (```) above and below code to denote code blocks. &emsp; <span style=\"color:green\">(1 Point)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3B. Line-by-line explain in your own words how this code works. Remember that you can use backticks \"\\`\" to denote `code`.&emsp; <span style=\"color:green\">(2 Points)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3C. Copy and paste the section of the code where the expected Q Values are calculated. Use 3 backticks (```) above and below code to denote code blocks.&emsp; <span style=\"color:green\">(1 Point)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3D. Line-by-line explain in your own words how this code works. Remember that you can use backticks \"\\`\" to denote `code`.&emsp; <span style=\"color:green\">(2 Points)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4 Changing Episode Length &emsp; <span style=\"color:green\">(1 Point)</span>\n",
        "\n",
        "Try different episode lengths. What happens when it is a lot shorter or a lot bigger?\n",
        "\n",
        "Hint: [catastrophic drop](https://ai.stackexchange.com/questions/28079/deep-q-learning-catastrophic-drop-reasons)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reinitialize Parameters\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "n_actions = env.action_space.n\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "steps_done = 0\n",
        "episode_durations = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make a new environment\n",
        "init_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "folder_name = os.path.join('video', 'CartPole-v1', f'{uuid.uuid4()}')\n",
        "env = RecordVideo(init_env, folder_name, disable_logger=True)\n",
        "\n",
        "# call the training loop\n",
        "train_loop(env, 50)  # Edit this!!!\n",
        "\n",
        "# close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5 Manipulating the Reward&emsp; <span style=\"color:green\">(10 Points Total)</span>\n",
        "\n",
        "Using [Gymnasium Reward Wrapper](https://gymnasium.farama.org/api/wrappers/reward_wrappers/) manipulate the reward in various ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.1 Set the reward to zero then run the training loop again. What happens? Why is that?&emsp; <span style=\"color:green\">(1 Point)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Answer #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.2 Set the reward to go from 1 every timestep, to negative after 100 timesteps. What changes in behavior does this cause in the actor? Why is that?&emsp; <span style=\"color:green\">(2 Point)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Answer #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.3.1 Currently we only are printing out the duration/iteration number. Before the environment-action loop initialize a variable `record_reward = []`, then during the loop append the reward after each action. Using that list, create a graph comparing the normal reward, zero reward, and negative after 100 reward with `matplotlib`. Make sure to label the axes, give the plots a title. After that, create another graph that takes the reward histories of all 3 reward policies and compares them on one graph, make sure to include a legend for this. \n",
        "\n",
        "(At least 4 graphs: 3 for each reward policy and 1 comparison graph of all three)\n",
        "\n",
        "matplotlib docs: https://matplotlib.org/stable/\n",
        "\n",
        "<span style=\"color:green\">(2 Points)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Answer #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.3.2 Create a new folder called `student_created_graphs`. Save the graphs you made in 5.3.1 (and all graphs you use in answers going forth), place them in this new folder, then display them using markdown `<img src=\"student_created_graphs/your_graph\">` in the cell below. &emsp; <span style=\"color:green\">(1 Point)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.4 Think of an interesting alternative reward transformation function that changes the behavior of the model in a noticeable way. What changed? Why did your modified reward cause this change? Make sure to display the plots/graphs you used to view this change. &emsp; <span style=\"color:green\">(2 Points)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Answer #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.5 Write a paragraph on how reward affects learning using what you've learned so far in this homework.&emsp; <span style=\"color:green\">(2 Points)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6 Manipulating the Observation&emsp; <span style=\"color:green\">(5 Points Total)</span>\n",
        "\n",
        "6.1 Using a Gymnasium [observation wrapper](https://gymnasium.farama.org/api/wrappers/observation_wrappers/) or directly zeroing out the indexes of the observation manipulate the observation so that the model can only see the angle of the pole. \n",
        "\n",
        "<span style=\"color:green\">(2 Points)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Answer #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.2 How does this affect learning (include graph)? &emsp; <span style=\"color:green\">(1 Point)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3 In theory, how should increasing/decreasing the obervation space affect learning? The current observations we are feeding to the model clearly include useful information. Are all observations equally useful? Could we theoretically add a useless observation? What are the tradeoffs of adding more observations? &emsp; <span style=\"color:green\">(1 Point)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.4 The current observation space is cart position, cart velocity, pole angle, and pole angular velocity. What if we changed the observation to the current frame, or a stack of frames of the environment? Reflect on in a single paragraph the benefits/detriments to doing this change, what factors are considered when choosing observations in general, and the importance of selecting a good observation space. &emsp; <span style=\"color:green\">(2 Points)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.5 Manipulating the observation changes what the model can see of the environment. What if we manipulate the environment itself? What could we change in the environment to make this cartpole task harder to learn what could we do to make it easier? &emsp; <span style=\"color:green\">(1 Point)</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 7 Spikes into Actions&emsp; <span style=\"color:green\">(10 Points Total)</span>\n",
        "\n",
        "In a future assigment instead of training a RL agent model, you will be training an organoid. As you learned in the electrophysiology homework, the output of an organoid is represented in spiketrains. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how to create a rate coded tensor by creating a tensor of probabilities and running a Bernouli trial over it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal Dynamics\n",
        "num_steps = 50\n",
        "\n",
        "# create vector filled with 0.5\n",
        "raw_vector = torch.ones(2, num_steps)*0.5\n",
        "\n",
        "# pass each sample through a Bernoulli trial\n",
        "rate_coded_vector = torch.bernoulli(raw_vector)\n",
        "print(f\"Converted vector: {rate_coded_vector}\")\n",
        "\n",
        "print(f\"The output is spiking {rate_coded_vector.sum()*100/torch.numel(rate_coded_vector):.2f}% of the time.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how to concatenate multiple tensors to make a artificial spike train:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_vector_1 = torch.ones(2, num_steps)*0.8\n",
        "raw_vector_2 = torch.ones(2, num_steps)*0.3\n",
        "raw_vector_sum = torch.cat((raw_vector_1, raw_vector_2))\n",
        "artificial_spike_train = torch.bernoulli(raw_vector_sum)\n",
        "print(artificial_spike_train)\n",
        "print(artificial_spike_train.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how to visualize the artificial spike train with a raster plot using `spikeplot.raster` from `snnTorch`:\n",
        "\n",
        "As you can see, \"neurons\" 0 and 1 are spiking 80% of the time, and \"neurons\" 2 and 3 are spiking 30% of the time.  \n",
        "\n",
        "Docs: https://snntorch.readthedocs.io/en/latest/snntorch.spikeplot.html#snntorch.spikeplot.raster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# transpose the spike train so that time is the first dimension\n",
        "artificial_spike_train = torch.transpose(artificial_spike_train, 0, 1)\n",
        "\n",
        "#  s: size of scatter points; c: color of scatter points\n",
        "splt.raster(artificial_spike_train, ax, s=1.5, c=\"black\")\n",
        "plt.title(\"Input Layer\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Neuron Number\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7.1 Ratio Threshold Decoding &emsp; <span style=\"color:green\">(2 Points)</span>\n",
        "\n",
        "One way to decode spikes into actions is to choose a a ratio threshold where if the ratio of spikes is greater than that ratio choose one action and if it is below that ratio choose a different action. This could also be used to determine whether or not to do an action. \n",
        "\n",
        "Fill in the following function that takes in a spike train and a ratio threshold (0-1). The function should calculate the ratio between the sum of all the spikes and the number of time steps multiplied by the number of neurons. If that ratio is less than or equal to the ratio threshold push the cart left (action 0), else push the cart to the right (action 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ratio_threshold(spike_train, ratio_threshold):\n",
        "    ###### Answer #####\n",
        "    action = _\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "folder_name = os.path.join('video', 'CartPole-v1', f'{uuid.uuid4()}')\n",
        "\n",
        "# Use the wrapper environment RecordVideo() to record video\n",
        "env = RecordVideo(init_env, folder_name, disable_logger=True)\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    new_spike_train = torch.bernoulli(\n",
        "        torch.cat((\n",
        "            torch.ones(2,num_steps) * 0.5,\n",
        "            torch.ones(2, num_steps) * 0.3\n",
        "        ))\n",
        "    )\n",
        "    action = ratio_threshold(new_spike_train, 0.5)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "show_video(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the videos\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(folder_name):\n",
        "    # Remove the directory\n",
        "    shutil.rmtree(folder_name)\n",
        "    print(\"Directory deleted successfully.\")\n",
        "else:\n",
        "    print(\"Directory does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7.2 Neuron Comparison Decoding&emsp; <span style=\"color:green\">(2 Points)</span>\n",
        "\n",
        "One way to decode spikes into actions is to compare the density or sum of spikes of groups of selected neurons. Then you would compare the sum or density to decide on an action.\n",
        "\n",
        "Fill in the following functions takes in a spike train and two lists indicating the neurons for the groups. The first function should compare spike sums, and the second function should compare spike density."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sum_comparison(spike_train, group1, group2):\n",
        "    ###### Answer #####\n",
        "    action = _\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def density_comparison(spike_train, group1, group2):\n",
        "    ###### Answer #####\n",
        "    action = _\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "folder_name = os.path.join('video', 'CartPole-v1', f'{uuid.uuid4()}')\n",
        "\n",
        "# Use the wrapper environment RecordVideo() to record video\n",
        "env = RecordVideo(init_env, folder_name, disable_logger=True)\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    new_spike_train = torch.bernoulli(\n",
        "        torch.cat((\n",
        "            torch.ones(2,num_steps) * 0.5,\n",
        "            torch.ones(2, num_steps) * 0.3\n",
        "        ))\n",
        "    )\n",
        "    action = sum_comparison(new_spike_train, [0,1], [2,3])\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "show_video(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the videos\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(folder_name):\n",
        "    # Remove the directory\n",
        "    shutil.rmtree(folder_name)\n",
        "    print(\"Directory deleted successfully.\")\n",
        "else:\n",
        "    print(\"Directory does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "folder_name = os.path.join('video', 'CartPole-v1', f'{uuid.uuid4()}')\n",
        "\n",
        "# Use the wrapper environment RecordVideo() to record video\n",
        "env = RecordVideo(init_env, folder_name, disable_logger=True)\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    new_spike_train = torch.bernoulli(\n",
        "        torch.cat((\n",
        "            torch.ones(2,num_steps) * 0.5,\n",
        "            torch.ones(2, num_steps) * 0.3\n",
        "        ))\n",
        "    )\n",
        "    action = sum_comparison(new_spike_train, [0,2], [1,3])\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "show_video(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the videos\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(folder_name):\n",
        "    # Remove the directory\n",
        "    shutil.rmtree(folder_name)\n",
        "    print(\"Directory deleted successfully.\")\n",
        "else:\n",
        "    print(\"Directory does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "folder_name = os.path.join('video', 'CartPole-v1', f'{uuid.uuid4()}')\n",
        "\n",
        "# Use the wrapper environment RecordVideo() to record video\n",
        "env = RecordVideo(init_env, folder_name, disable_logger=True)\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    new_spike_train = torch.bernoulli(\n",
        "        torch.cat((\n",
        "            torch.ones(2,num_steps) * 0.5,\n",
        "            torch.ones(2, num_steps) * 0.3\n",
        "        ))\n",
        "    )\n",
        "    action = density_comparison(new_spike_train, [0], [1,2,3])\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "show_video(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the videos\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(folder_name):\n",
        "    # Remove the directory\n",
        "    shutil.rmtree(folder_name)\n",
        "    print(\"Directory deleted successfully.\")\n",
        "else:\n",
        "    print(\"Directory does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7.3 Stochastic Spike Decoding&emsp; <span style=\"color:green\">(2 Points)</span>\n",
        "\n",
        "Instead of comparing the spike densities of groups of neurons, one could interpret the density of a subset of the spike train as a probability to take an action. \n",
        "\n",
        "Fill in the following function that takes in a spike train, and a list of neurons in a group. It then calculates the density of that group and then uses that probability to determine if the cart pole should move right (1). That is if the density is 0.9, there is a 90% chance the cart will be moved right, else move the cart left (0). \n",
        "\n",
        "Hint: You can use `random.random()` to generate a random number between 0.0 and 1.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sum_comparison(spike_train, group):\n",
        "    ###### Answer #####\n",
        "    action = _\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "folder_name = os.path.join('video', 'CartPole-v1', f'{uuid.uuid4()}')\n",
        "\n",
        "# Use the wrapper environment RecordVideo() to record video\n",
        "env = RecordVideo(init_env, folder_name, disable_logger=True)\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    new_spike_train = torch.bernoulli(\n",
        "        torch.cat((\n",
        "            torch.ones(2,num_steps) * 0.5,\n",
        "            torch.ones(2, num_steps) * 0.3\n",
        "        ))\n",
        "    )\n",
        "    action = density_comparison(new_spike_train, [0, 1, 2])\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "show_video(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the videos\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(folder_name):\n",
        "    # Remove the directory\n",
        "    shutil.rmtree(folder_name)\n",
        "    print(\"Directory deleted successfully.\")\n",
        "else:\n",
        "    print(\"Directory does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7.4 Decode Into a Continuous Action Space&emsp; <span style=\"color:green\">(1 Points)</span>\n",
        "\n",
        "So far we have been working with binary descrete actions, but it is possible to not only have multiple actions but a continuous action space instead. For example, if you wanted to determine the degrees a agent controlling a top down character mouse should move. A continuous action space would include not just the 360 degree angles, but even the decimal points in between them. \n",
        "\n",
        "Create a function that takes in a spike train and outputs a angle from 0-360 a theoretical mouse should move by returning the product of density and 360."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def continuous_degrees_turned(spike_train):\n",
        "    ##### Answer ######\n",
        "    action = _\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "angle = continuous_degrees_turned(artificial_spike_train)\n",
        "print(angle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7.5 Do it yourself!&emsp; <span style=\"color:green\">(3 Points)</span>\n",
        "\n",
        "Create your own spike decoding to action function by combining the various techniques we just showed you or something else entirely! Then show that it works as expected by feeding a spike train into it. We want you to be creative and think about how you will decode the spikes in the upcoming project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Answer #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feedback (Optional)\n",
        "\n",
        "This is a new homework for this quarter, plase provide feedback on things you think could be improved or even if you like it as is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\">Answer</span>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
